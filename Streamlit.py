# ğŸ“Š YouTube-Ğ°Ğ½Ğ°Ğ»Ñ–Ñ‚Ğ¸ĞºĞ° Ñƒ Streamlit (Ğ±ĞµĞ· seaborn, matplotlib, datetime)
import streamlit as st
import pandas as pd
import numpy as np

st.set_page_config(page_title="YouTube Analytics", layout="wide")
st.title("ğŸ“Š YouTube ĞĞ½Ğ°Ğ»Ñ–Ñ‚Ğ¸ĞºĞ° ĞºĞ°Ğ½Ğ°Ğ»Ñ–Ğ²")

# === 1ï¸âƒ£ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ CSV Ğ°Ğ±Ğ¾ Ğ´ĞµĞ¼Ğ¾ ===
uploaded = st.file_uploader("Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ CSV (title, views, comments, likes, date, content_type)", type=["csv"])

if uploaded:
    df = pd.read_csv(uploaded)
    st.success("âœ… Ğ”Ğ°Ğ½Ñ– Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾!")
else:
    st.info("âš™ï¸ Ğ”Ğ°Ğ½Ñ– Ğ½Ğµ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾ â€” ÑÑ‚Ğ²Ğ¾Ñ€ÑÑ Ğ´ĞµĞ¼Ğ¾-Ğ½Ğ°Ğ±Ñ–Ñ€...")
    np.random.seed(1)
    n = 200
    # ÑÑ‚Ğ²Ğ¾Ñ€Ğ¸Ğ¼Ğ¾ Ğ´Ğ°Ñ‚Ğ¸ ÑĞº Ñ€ÑĞ´ĞºĞ¸ (Ğ±ĞµĞ· datetime)
    days = np.random.choice(pd.date_range("2025-01-01", "2025-10-01").astype(str), n)
    hours = np.random.randint(0, 24, n)
    df = pd.DataFrame({
        "title": [f"Video {i+1}" for i in range(n)],
        "views": np.random.randint(100, 20000, n),
        "comments": np.random.randint(0, 500, n),
        "likes": np.random.randint(0, 3000, n),
        "date": [f"{d} {h:02d}:00:00" for d, h in zip(days, hours)],
        "content_type": np.random.choice(["Shorts", "Long-form", "Stream"], n)
    })

# === 2ï¸âƒ£ ĞĞ±Ñ€Ğ¾Ğ±ĞºĞ° Ğ´Ğ°Ğ½Ğ¸Ñ… ===
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df["day_of_week"] = df["date"].dt.day_name()
df["hour"] = df["date"].dt.hour

# === 3ï¸âƒ£ Ğ¤Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸ ===
col1, col2, col3 = st.columns(3)
with col1:
    min_date, max_date = df["date"].min(), df["date"].max()
    start_date = st.date_input("ĞŸĞ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°", min_date)
with col2:
    end_date = st.date_input("ĞšÑ–Ğ½Ñ†ĞµĞ²Ğ° Ğ´Ğ°Ñ‚Ğ°", max_date)
with col3:
    content_filter = st.selectbox("Ğ¢Ğ¸Ğ¿ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ñƒ", ["Ğ£ÑÑ–"] + sorted(df["content_type"].unique()))

filtered = df[
    (df["date"] >= pd.to_datetime(start_date))
    & (df["date"] <= pd.to_datetime(end_date))
]
if content_filter != "Ğ£ÑÑ–":
    filtered = filtered[filtered["content_type"] == content_filter]

# === 4ï¸âƒ£ Engagement rate ===
filtered["engagement_rate"] = ((filtered["likes"] + filtered["comments"]) / filtered["views"].replace(0, np.nan)) * 100
st.metric("ğŸ“ˆ Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ–Ğ¹ engagement rate", f"{filtered['engagement_rate'].mean():.2f}%")
st.write(f"Ğ’Ñ–Ğ´ĞµĞ¾ Ñƒ Ğ²Ğ¸Ğ±Ñ–Ñ€Ñ†Ñ–: **{len(filtered)}**")

# === 5ï¸âƒ£ Heatmap Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ñ– (Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾) ===
st.subheader("ğŸ”¥ Heatmap Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ñ– (ÑĞµÑ€ĞµĞ´Ğ½Ñ– Ğ¿ĞµÑ€ĞµĞ³Ğ»ÑĞ´Ğ¸)")
pivot = filtered.groupby(["day_of_week", "hour"])["views"].mean().unstack(fill_value=0)
pivot = pivot.reindex(["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"])
st.dataframe(pivot.style.background_gradient(cmap="YlOrRd"))

# === 6ï¸âƒ£ Ğ¢Ğ¾Ğ¿-10 Ğ²Ñ–Ğ´ĞµĞ¾ Ğ·Ğ° engagement rate ===
st.subheader("ğŸ† Ğ¢Ğ¾Ğ¿-10 Ğ²Ñ–Ğ´ĞµĞ¾ Ğ·Ğ° engagement rate")
top10 = filtered.sort_values("engagement_rate", ascending=False).head(10)
st.dataframe(top10[["title","views","likes","comments","engagement_rate"]])

# === 7ï¸âƒ£ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² ===
csv = filtered.to_csv(index=False).encode("utf-8")
st.download_button("ğŸ’¾ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ (CSV)", csv, "youtube_analysis.csv", "text/csv")

st.success("âœ… ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
